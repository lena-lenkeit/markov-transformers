import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from einops import einsum, rearrange, repeat
from jaxtyping import Float, Int
from tqdm.auto import trange

from markov.predict.torch import (
    get_stationary_distribution,
    propagate_values,
    update_posterior,
)


def hmm_from_residuals(
    residuals: Float[torch.Tensor, "batch sequence residual"],
    observations: Int[torch.Tensor, "batch sequence"],
    dim_state: int,
    dim_observations: int,
    num_train_steps: int = 1024,
):
    """Attempts to decode the full HMM (transition and emission matrix) given sequences
    of belief states and observations by assuming the belief states
    were generated by an optimal predictor."""

    # TODO: Optional auto-estimation of state size from PCA/SVD of residuals
    # TODO: Also auto-initialization of mapping from PCA/SVD of residuals
    # !!!TODO: Find an alternative for the softmax, if that is mathematically possible

    dim_batch, dim_sequence, dim_residual = residuals.shape

    transition_logit_matrix = torch.empty((dim_state, dim_state), requires_grad=True)
    emission_logit_matrix = torch.empty(
        (dim_state, dim_observations), requires_grad=True
    )

    residual_belief_mapping = nn.Linear(dim_residual, dim_state)

    with torch.no_grad():
        transition_logit_matrix.data.normal_(std=1e-1)
        emission_logit_matrix.data.normal_(std=1e-1)

    optimizer = optim.AdamW(
        [transition_logit_matrix, emission_logit_matrix]
        + list(residual_belief_mapping.parameters()),
        lr=1e-2,
        weight_decay=0.0,
    )

    pbar = trange(num_train_steps)
    for i in pbar:
        transition_matrix = F.softmax(transition_logit_matrix, dim=1)
        emission_matrix = F.softmax(emission_logit_matrix, dim=1)

        belief_states = residual_belief_mapping(residuals)
        # belief_states = F.softmax(belief_states, dim=-1)
        min_belief = torch.min(belief_states, dim=-1, keepdim=True).values
        belief_states = belief_states - min_belief.clamp(max=0.0)
        belief_states = belief_states / torch.sum(belief_states, dim=-1, keepdim=True)

        current_probs = einsum(belief_states, emission_matrix, "... i, i j -> ... j")
        current_log_probs = torch.log(current_probs)

        next_belief_states = update_posterior(
            belief_states, observations, emission_matrix
        )
        next_belief_states = propagate_values(
            next_belief_states, transition_matrix, normalize=True
        )

        next_probs = einsum(next_belief_states, emission_matrix, "... i, i j -> ... j")
        next_log_probs = torch.log(next_probs)

        loss = (
            F.nll_loss(
                rearrange(
                    current_log_probs,
                    "batch sequence logits -> (batch sequence) logits",
                ),
                rearrange(observations, "batch sequence -> (batch sequence)"),
            )
            * 0.5
            + F.nll_loss(
                rearrange(
                    next_log_probs[:, :-1],
                    "batch sequence logits -> (batch sequence) logits",
                ),
                rearrange(observations[:, 1:], "batch sequence -> (batch sequence)"),
            )
            * 0.5
        )

        optimizer.zero_grad(set_to_none=True)
        loss.backward()
        optimizer.step()

        pbar.set_postfix_str(f"Loss: {loss:.2e}")

    with torch.no_grad():
        transition_matrix = F.softmax(transition_logit_matrix, dim=1)
        emission_matrix = F.softmax(emission_logit_matrix, dim=1)
        prior = get_stationary_distribution(transition_matrix)

    return transition_matrix, emission_matrix, prior, residual_belief_mapping


def hmm_from_belief_states(
    belief_states: Float[torch.Tensor, "batch sequence state"],
    observations: Int[torch.Tensor, "batch sequence"],
    dim_observations: int,
):
    """Attempts to decode the full HMM (transition and emission matrix) given sequences
    of belief states and observations by assuming the belief states
    were generated by an optimal predictor."""

    dim_batch, dim_sequence, dim_state = belief_states.shape

    transition_logit_matrix = torch.empty((dim_state, dim_state), requires_grad=True)
    emission_logit_matrix = torch.empty(
        (dim_state, dim_observations), requires_grad=True
    )

    with torch.no_grad():
        transition_logit_matrix.data.normal_(std=1e-1)
        emission_logit_matrix.data.normal_(std=1e-1)

    optimizer = optim.AdamW(
        [transition_logit_matrix, emission_logit_matrix], lr=1e-1, weight_decay=0.0
    )

    pbar = trange(256)
    for i in pbar:
        transition_matrix = F.softmax(transition_logit_matrix, dim=1)
        emission_matrix = F.softmax(emission_logit_matrix, dim=1)

        current_probs = einsum(belief_states, emission_matrix, "... i, i j -> ... j")
        current_log_probs = torch.log(current_probs)

        next_belief_states = update_posterior(
            belief_states, observations, emission_matrix
        )
        next_belief_states = propagate_values(
            next_belief_states, transition_matrix, normalize=True
        )

        next_probs = einsum(next_belief_states, emission_matrix, "... i, i j -> ... j")
        next_log_probs = torch.log(next_probs)

        loss = (
            F.nll_loss(
                rearrange(
                    current_log_probs,
                    "batch sequence logits -> (batch sequence) logits",
                ),
                rearrange(observations, "batch sequence -> (batch sequence)"),
            )
            * 0.5
            + F.nll_loss(
                rearrange(
                    next_log_probs[:, :-1],
                    "batch sequence logits -> (batch sequence) logits",
                ),
                rearrange(observations[:, 1:], "batch sequence -> (batch sequence)"),
            )
            * 0.5
        )

        optimizer.zero_grad(set_to_none=True)
        loss.backward()
        optimizer.step()

        pbar.set_postfix_str(f"Loss: {loss:.2e}")

    with torch.no_grad():
        transition_matrix = F.softmax(transition_logit_matrix, dim=1)
        emission_matrix = F.softmax(emission_logit_matrix, dim=1)
        prior = get_stationary_distribution(transition_matrix)

    return transition_matrix, emission_matrix, prior


def hmm_from_observations(
    observations: Int[torch.Tensor, "batch sequence"],
    dim_state: int,
    dim_observations: int,
):
    dim_batch, dim_sequence = observations.shape

    transition_logit_matrix = torch.empty((dim_state, dim_state), requires_grad=True)
    emission_logit_matrix = torch.empty(
        (dim_state, dim_observations), requires_grad=True
    )

    with torch.no_grad():
        transition_logit_matrix.data.normal_(std=1e-1)
        emission_logit_matrix.data.normal_(std=1e-1)

    optimizer = optim.AdamW(
        [transition_logit_matrix, emission_logit_matrix], lr=1e-1, weight_decay=0.0
    )

    pbar = trange(256)
    for i in pbar:
        transition_matrix = F.softmax(transition_logit_matrix, dim=1)
        emission_matrix = F.softmax(emission_logit_matrix, dim=1)
        prior = get_stationary_distribution(transition_matrix)

        # Get initial state
        belief_state = repeat(prior, "states -> batch states", batch=dim_batch)
        probs = einsum(belief_state, emission_matrix, "... i, i j -> ... j")

        seq_probs = [probs]
        for j in range(dim_sequence - 1):
            tokens = observations[:, j]

            belief_state = update_posterior(belief_state, tokens, emission_matrix)
            belief_state = propagate_values(
                belief_state, transition_matrix, normalize=True
            )

            probs = einsum(belief_state, emission_matrix, "... i, i j -> ... j")
            seq_probs.append(probs)

        seq_probs = torch.stack(seq_probs, dim=1)
        seq_log_probs = torch.log(seq_probs)
        loss = F.nll_loss(
            rearrange(
                seq_log_probs, "batch sequence logits -> (batch sequence) logits"
            ),
            rearrange(observations, "batch sequence -> (batch sequence)"),
        )

        optimizer.zero_grad(set_to_none=True)
        loss.backward()
        optimizer.step()

        pbar.set_postfix_str(f"Loss: {loss:.2e}")

    with torch.no_grad():
        transition_matrix = F.softmax(transition_logit_matrix, dim=1)
        emission_matrix = F.softmax(emission_logit_matrix, dim=1)
        prior = get_stationary_distribution(transition_matrix)

    return transition_matrix, emission_matrix, prior
